"use strict";(self.webpackChunksynalinks_website=self.webpackChunksynalinks_website||[]).push([[953],{586:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>m});var d=t(4848),s=t(8453);const i={sidebar_position:1},o="Sentence Transformer Embeddings",r={id:"api/embeddings/sentence-embeddings",title:"Sentence Transformer Embeddings",description:"Embeddings for text data",source:"@site/docs/api/embeddings/sentence-embeddings.md",sourceDirName:"api/embeddings",slug:"/api/embeddings/sentence-embeddings",permalink:"/documentation/docs/api/embeddings/sentence-embeddings",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"apiSidebar",previous:{title:"Embeddings",permalink:"/documentation/docs/category/embeddings"},next:{title:"Fake Embeddings",permalink:"/documentation/docs/api/embeddings/fake-embeddings"}},a={},m=[{value:"Embeddings for text data",id:"embeddings-for-text-data",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.h1,{id:"sentence-transformer-embeddings",children:"Sentence Transformer Embeddings"}),"\n",(0,d.jsx)(n.h2,{id:"embeddings-for-text-data",children:"Embeddings for text data"}),"\n",(0,d.jsxs)(n.p,{children:["Sentence Transformer Embeddings are used for computing the vectors used in similarity retrieval. These embeddings are an essential component for each ",(0,d.jsx)(n.code,{children:"HybridStore"}),", empowering you to fetch text data efficiently."]}),"\n",(0,d.jsx)(n.h1,{id:"usage",children:"Usage"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{className:"language-python",children:'from hybridagi import SentenceTransformerEmbeddings\n\nembeddings = SentenceTransformerEmbeddings(\n    model_name_or_path = "sentence-transformers/all-MiniLM-L6-v2", # The name of the model to use\n    dim = 384, # The dimension of the embeddings vector\n    max_gpu_devices = 1, # The maximum number of GPU to use (default to 1)\n    batch_size = 256, # The maximum of embeddings to compute in one batch (default to 256)\n    max_seq_length = 256, # The maximum number of input tokens for the embeddings (default to 256) \n    normalize_embeddings = True, # Whether or not to normalize the embeddings (default to True)\n)\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var d=t(6540);const s={},i=d.createContext(s);function o(e){const n=d.useContext(i);return d.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),d.createElement(i.Provider,{value:n},e.children)}}}]);